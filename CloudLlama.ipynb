{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c1ea03a-cc69-45b0-80d3-664e48ca6831",
   "metadata": {},
   "source": [
    "## This notebook contains:\n",
    "* Running Llama2 in the cloud hosted on Replicate\n",
    "* Using LangChain to ask Llama general questions and follow up questions\n",
    "* Using LangChain to load PDF docs - (Kenya Law Documents) - and chat about it.\n",
    "* The end result will be a chatbot that will be able answer questions about the data not publicly available when Llama2 was trained, or about your own data. RAG is one way to prevent LLM's hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dde626",
   "metadata": {},
   "source": [
    "Let's start by installing the necessary packages:\n",
    "- sentence-transformers for text embeddings\n",
    "- chromadb gives us database capabilities \n",
    "- langchain provides necessary RAG tools for this demo\n",
    "\n",
    "And setting up the Replicate token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c608df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain replicate sentence-transformers chromadb pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8870c1",
   "metadata": {},
   "source": [
    "Next we call the Llama 2 model from replicate. In this example we will use the llama 2 13b chat model. You can find more Llama 2 models by searching for them on the [Replicate model explore page](https://replicate.com/explore?query=llama).\n",
    "\n",
    "The model is added in the format: model_name/version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb042eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Replicate API token...\n",
      "Replicate API token set.\n",
      "Initializing Llama2 model...\n",
      "Llama2 model initialized.\n"
     ]
    }
   ],
   "source": [
    "# set up the Replicate API token\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the environment variable\n",
    "REPLICATE_API_TOKEN = os.getenv('REPLICATE_API_TOKEN')\n",
    "\n",
    "print(\"Getting Replicate API token...\")\n",
    "REPLICATE_API_TOKEN = \"r8_9HSSIir2ZgP4D6iPNvE7H8JU4ejBMkC0TssZD\" \n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
    "print(\"Replicate API token set.\")\n",
    "\n",
    "# initialize the Llama2 model\n",
    "\n",
    "from langchain.llms import Replicate\n",
    "\n",
    "print(\"Initializing Llama2 model...\")\n",
    "llama2_13b = \"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\"\n",
    "llm = Replicate(\n",
    "    model=llama2_13b,\n",
    "    model_kwargs={\"temperature\": 0.01, \"top_p\": 1, \"max_new_tokens\":500}\n",
    ")\n",
    "print(\"Llama2 model initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd207c80",
   "metadata": {},
   "source": [
    "With the model set up, it is now possible to ask some questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeaffc7",
   "metadata": {},
   "source": [
    "[`ConversationBufferMemory`](https://python.langchain.com/docs/modules/memory/types/buffer) is used to pass the chat history to the model and give it the capability to handle follow up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5428ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ConversationBufferMemory to pass memory (chat history) for follow up questions\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e9af5f",
   "metadata": {},
   "source": [
    "Once this is set up, let us repeat the steps from before and ask the model a simple question.\n",
    "\n",
    "Then we pass the question and answer back into the model for context along with the follow up question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99516a1a",
   "metadata": {},
   "source": [
    "Load the documents from the data path\n",
    "\n",
    "Using Llama 2 to answer questions using documents for context. \n",
    "This gives us the ability to update Llama 2's knowledge thus giving it better context without needing to fine-tune. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61ceb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "DATA_PATH = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f79ad922",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2cf3f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PDF files: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check the number of PDF files in the data directory\n",
    "pdf_files = [f for f in os.listdir(DATA_PATH) if f.endswith('.pdf')]\n",
    "print(f\"Number of PDF files: {len(pdf_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8268e",
   "metadata": {},
   "source": [
    "Storing the documents. There are more than 30 vector stores (DBs) supported by LangChain. \n",
    "In this case [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma) is used which is light-weight and in memory so it's easy to get started with.\n",
    "\n",
    "We will also import the HuggingFaceEmbeddings and RecursiveCharacterTextSplitter to assist in storing the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eecb6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# embeddings are numerical representations of the question and answer text\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# use a common text splitter to split text into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4a17c",
   "metadata": {},
   "source": [
    "To store the documents, we will need to split them into chunks using [`RecursiveCharacterTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter) and create vector representations of these chunks using [`HuggingFaceEmbeddings`](https://www.google.com/search?q=langchain+hugging+face+embeddings&sca_esv=572890011&ei=ARUoZaH4LuumptQP48ah2Ac&oq=langchian+hugg&gs_lp=Egxnd3Mtd2l6LXNlcnAiDmxhbmdjaGlhbiBodWdnKgIIADIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCkjeHlC5Cli5D3ABeAGQAQCYAV6gAb4CqgEBNLgBAcgBAPgBAcICChAAGEcY1gQYsAPiAwQYACBBiAYBkAYI&sclient=gws-wiz-serp) on them before storing them into our vector database. \n",
    "\n",
    "In general, you should use larger chuck sizes for highly structured text such as code and smaller size for less structured text. You may need to experiment with different chunk sizes and overlap values to find out the best numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc65e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# create the vector db to store all the split chunks as embeddings\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad02d7",
   "metadata": {},
   "source": [
    "We then use ` RetrievalQA` to retrieve the documents from the vector database and give the model more context on Llama 2, thereby increasing its knowledge.\n",
    "\n",
    "For each question, LangChain performs a semantic similarity search of it in the vector db, then passes the search results as the context to Llama to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00e3f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LangChain's RetrievalQA, to associate Llama with the loaded documents stored in the vector db\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c5ca7",
   "metadata": {},
   "source": [
    "System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94f80415",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a medical doctor trying to investigate and come up with the next investigation plan both lab and imaging for the patient specified and fill gaps in history. Provide additional questions for the history and suitable medical investigation that would lead you to a diagnosis.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c4326",
   "metadata": {},
   "source": [
    "First Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "596eb67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ezra/miniforge3/envs/cloud_llama/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the information provided, the following investigations may be relevant for the patient W.F to come up with a diagnosis:\n",
      "\n",
      "1. Complete Blood Count (CBC) and Differential: To assess for signs of infection, inflammation, and blood cell counts.\n",
      "2. Electrolyte Panel: To evaluate for any electrolyte imbalances that may be contributing to the patient's symptoms.\n",
      "3. Renal Function Tests: To assess for any renal impairment or damage.\n",
      "4. Liver Function Tests: To evaluate for any liver dysfunction or injury.\n",
      "5. HIV Viral Load and CD4 Count: To monitor the patient's HIV status and determine if there is any progression of the disease.\n",
      "6. Throat Swab Culture: To identify any bacterial or viral infections causing the sore throat.\n",
      "7. Blood Cultures: To detect any bacteremia or sepsis.\n",
      "8. Urinalysis: To evaluate for any urinary tract infections or inflammation.\n",
      "9. Chest X-ray: To rule out any respiratory infections or complications.\n",
      "10. Erythrocyte Sedimentation Rate (ESR) and C-Reactive Protein (CRP): To assess for any inflammatory activity in the body.\n",
      "\n",
      "These investigations can help to identify the underlying cause of the patient's symptoms and guide appropriate treatment. However, it is important to note that these are just suggestions based on the information provided, and a thorough clinical evaluation and consultation with a healthcare professional should be done before starting any investigations.\n"
     ]
    }
   ],
   "source": [
    "question = \"What investigations are relevant for the patient W.F to come up with a diagnosis?\"\n",
    "# Query the model with the question and system prompt\n",
    "result = qa_chain({\"query\": question, \"system_prompt\": system_prompt})\n",
    "print(result['result'])\n",
    "# Save the question and answer to the chat history\n",
    "chat_history = [(question, result[\"result\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833221c0",
   "metadata": {},
   "source": [
    "As we did before, let us use the `ConversationalRetrievalChain` package to give the model context of our previous question so we can add follow up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7ac9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ConversationalRetrievalChain to pass chat history for follow up questions\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "chat_chain = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb4e6e",
   "metadata": {},
   "source": [
    "Follow Up Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4b3406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As a helpful assistant, I will do my best to provide an answer based on the given context. However, please note that I'm just an AI and not a medical professional, so my response should not be taken as medical advice.\n",
      "\n",
      "Based on the information provided, it seems that patient J.O is presenting with symptoms consistent with meningitis, such as stiff neck, headache, photophobia, and nausea. A lumbar puncture (LP) would be a valuable diagnostic tool in evaluating the patient's condition.\n",
      "\n",
      "The expected findings from an LP include:\n",
      "\n",
      "1. Cerebrospinal fluid (CSF) analysis: The CSF sampled during the LP can be analyzed for various parameters, including white blood cell count, protein levels, glucose levels, and bacterial cultures. Abnormalities in these values can help support a diagnosis of meningitis.\n",
      "2. Cell count: An elevated white blood cell count in the CSF may indicate inflammation or infection in the central nervous system.\n",
      "3. Protein levels: Elevated protein levels in the CSF can suggest meningeal irritation or inflammation.\n",
      "4. Glucose levels: Low glucose levels in the CSF can indicate bacterial infection.\n",
      "5. Bacterial cultures: Bacterial cultures can identify the presence of specific pathogens in the CSF, which can guide antibiotic selection and treatment.\n",
      "6. Imaging: LP can also aid in identifying any structural abnormalities in the brain or spinal cord.\n",
      "\n",
      "These findings can contribute to the patient's diagnosis and treatment plan by helping to confirm or rule out potential causes of their symptoms. For example, if the LP reveals bacterial meningitis, this information can guide antibiotic selection and treatment. Additionally, the results of the LP can help to identify any underlying conditions that may need to be addressed as part of the patient's overall treatment plan.\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question\n",
    "follow_up_question = \"What findings do we expect in Lumbar puncture for the patient J.O?\"\n",
    "# Append the follow-up question to the chat history\n",
    "chat_history.append((follow_up_question, \"\"))\n",
    "# Query the model with the follow-up question, chat history, and system prompt\n",
    "followup_answer = chat_chain({\"question\": follow_up_question, \"chat_history\": chat_history, \"system_prompt\": system_prompt})\n",
    "\n",
    "print(followup_answer['answer'])  # Changed 'result' to 'answer'\n",
    "\n",
    "# Append the follow-up answer to the chat history\n",
    "chat_history.append((follow_up_question, followup_answer['answer']))  # Changed 'result' to 'answer'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question\n",
    "follow_up_question = \"What additional questions in the history would be relevant for the patient J.O in coming up with a diagnosis?\"\n",
    "# Append the follow-up question to the chat history\n",
    "chat_history.append((follow_up_question, \"\"))\n",
    "# Query the model with the follow-up question, chat history, and system prompt\n",
    "followup_answer = chat_chain({\"question\": follow_up_question, \"chat_history\": chat_history, \"system_prompt\": system_prompt})\n",
    "\n",
    "print(followup_answer['answer'])  # Changed 'result' to 'answer'\n",
    "\n",
    "# Append the follow-up answer to the chat history\n",
    "chat_history.append((follow_up_question, followup_answer['answer']))  # Changed 'result' to 'answer'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
